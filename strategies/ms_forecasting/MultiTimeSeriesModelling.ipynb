{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Commented a bunch of stuff out to speed up\n",
   "id": "a75b1dbe6506b8b9"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from skforecast.plot import set_dark_theme\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sklearn\n",
    "import skforecast\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from skforecast.recursive import ForecasterRecursive, ForecasterRecursiveMultiSeries\n",
    "from skforecast.model_selection import (\n",
    "    TimeSeriesFold,\n",
    "    OneStepAheadFold,\n",
    "    backtesting_forecaster,\n",
    "    bayesian_search_forecaster,\n",
    "    backtesting_forecaster_multiseries,\n",
    "    bayesian_search_forecaster_multiseries\n",
    ")\n",
    "from skforecast.preprocessing import RollingFeatures, series_long_to_dict, exog_long_to_dict\n",
    "from skforecast.exceptions import OneStepAheadValidationWarning\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ResourceWarning)\n",
    "\n",
    "colourOrangeBold = \"\\033[1m\\033[38;5;208m\"\n",
    "colourReset = \"\\033[0m\"\n",
    "\n",
    "print(f\"{colourOrangeBold}Version skforecast: {skforecast.__version__}{colourReset}\")\n",
    "print(f\"{colourOrangeBold}Version scikit-learn: {sklearn.__version__}{colourReset}\")\n",
    "print(f\"{colourOrangeBold}Version pandas: {pd.__version__}{colourReset}\")\n",
    "print(f\"{colourOrangeBold}Version numpy: {np.__version__}{colourReset}\")\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path().resolve()\n",
    "while project_root.name != \"Algothon-2025\" and project_root != project_root.parent:\n",
    "    project_root = project_root.parent\n",
    "\n",
    "os.chdir(project_root)\n",
    "print(\"Working directory set to:\", os.getcwd())\n",
    "\n",
    "set_dark_theme()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Set Training, Cross Validation and Testing Splits",
   "id": "2956987ad8a71855"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "TRAIN_START = 20\n",
    "TRAIN_END = 600\n",
    "VAL_END = 675"
   ],
   "id": "cc2c5e64691bc739",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Import our price data: Prices are what we are predicting",
   "id": "ac407001988f327d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# PRICES DATA:\n",
    "data = pd.read_csv(\"./sourceCode/prices.txt\", sep=r'\\s+', header=None)\n",
    "print(f\"{colourOrangeBold}Shape: {data.shape}{colourReset}\")"
   ],
   "id": "85720b28a61a95c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Add column names to prices data",
   "id": "e888e4500c137e28"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data.index.name = 'day'\n",
    "data.columns = [f\"inst_{i}\" for i in range(data.shape[1])]"
   ],
   "id": "dde103d431db9c16",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Split data into correct splits",
   "id": "7805e53d5e095549"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# DATA SPLITTING:\n",
    "data_train = data.loc[TRAIN_START:TRAIN_END - 1].copy()\n",
    "data_val   = data.loc[TRAIN_END:VAL_END - 1].copy()\n",
    "data_test  = data.loc[VAL_END:].copy()\n",
    "\n",
    "print(f\"Train days      : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\")\n",
    "print(f\"Validation days : {data_val.index.min()} --- {data_val.index.max()}  (n={len(data_val)})\")\n",
    "print(f\"Test days       : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\")"
   ],
   "id": "2398165cf1c07a3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Plot prices data for a few instruments for fun",
   "id": "b1750dadb4ce722"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "instrumentCount = 4\n",
    "\n",
    "fig, axs = plt.subplots(instrumentCount, 1, figsize=(7, 5), sharex=True)\n",
    "data.iloc[:, :instrumentCount].plot(\n",
    "    legend=True,\n",
    "    subplots=True,\n",
    "    title='First 4 Instruments Prices Over Time',\n",
    "    ax=axs,\n",
    "    linewidth=1\n",
    ")\n",
    "# Add vertical lines at training and validation split\n",
    "for ax in axs:\n",
    "    ax.axvline(x=TRAIN_END, color='white', linestyle='--', linewidth=1)\n",
    "    ax.axvline(x=VAL_END, color='white', linestyle='--', linewidth=1)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "id": "548c373ad4fbb1b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Train a forecaster that predicts prices for individual instruments for a baseline reference.",
   "id": "4814230e0b310605"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "items = []\n",
    "mse_values = []\n",
    "predictions = {}\n",
    "\n",
    "for i, item in enumerate(tqdm(data.columns)):\n",
    "    # Define forecaster\n",
    "    window_features = RollingFeatures(stats=['mean', 'min', 'max'], window_sizes=7)\n",
    "\n",
    "    forecaster = ForecasterRecursive(\n",
    "        regressor=HistGradientBoostingRegressor(random_state=8523),\n",
    "        lags=20,\n",
    "        window_features=window_features\n",
    "    )\n",
    "\n",
    "    # Backtesting forecaster\n",
    "    cv = TimeSeriesFold(\n",
    "        steps=1,\n",
    "        initial_train_size=len(data_train) + len(data_val),\n",
    "        refit=False,\n",
    "    )\n",
    "\n",
    "    metric, preds = backtesting_forecaster(\n",
    "        forecaster=forecaster,\n",
    "        y=data[item],\n",
    "        cv=cv,\n",
    "        metric='mean_squared_error',\n",
    "        show_progress=False\n",
    "    )\n",
    "\n",
    "    items.append(item)\n",
    "    mse_values.append(metric.at[0, 'mean_squared_error'])\n",
    "    predictions[item] = preds\n",
    "\n",
    "# Results\n",
    "uni_series_mse = pd.Series(\n",
    "    data=mse_values,\n",
    "    index=items,\n",
    "    name='uni_series_mse'\n",
    ")\n",
    "\n",
    "print(f\"{colourOrangeBold}========================{colourReset}\")\n",
    "print(uni_series_mse.head())\n",
    "print(f\"{colourOrangeBold}========================{colourReset}\\n\\n\")\n"
   ],
   "id": "3475bfbbe6ebe270",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Train a global / multi time series predictor that takes into account instrument correlation (Evaluated with the same backtester as the single instrument predictor):",
   "id": "4243b997dd186b52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "items = list(data.columns)\n",
    "\n",
    "# Define forecaster\n",
    "window_features = RollingFeatures(stats=['mean', 'min', 'max'], window_sizes=14)\n",
    "forecaster_ms = ForecasterRecursiveMultiSeries(\n",
    "    regressor           =GradientBoostingRegressor(random_state=8523), # ~ 20 seconds\n",
    "    # regressor           =HistGradientBoostingRegressor(random_state=8523), # Much quicker, slightly less accurate, nan safe\n",
    "    lags                =20,\n",
    "    encoding            ='ordinal',\n",
    "    transformer_series  =StandardScaler(),\n",
    "    window_features     =window_features,\n",
    ")\n",
    "\n",
    "# Backtesting forecaster for all items\n",
    "cv = TimeSeriesFold(\n",
    "    initial_train_size  =len(data_train) + len(data_val),\n",
    "    steps               =1,   # Predict 7 steps ahead\n",
    "    refit               =True # Retrain at each split\n",
    ")\n",
    "\n",
    "multi_series_mse, predictions_ms = backtesting_forecaster_multiseries(\n",
    "    forecaster          =forecaster_ms,\n",
    "    series              =data,\n",
    "    levels              =items,\n",
    "    cv                  =cv,\n",
    "    metric              ='mean_squared_error',\n",
    ")\n",
    "\n",
    "# Results\n",
    "display(multi_series_mse.head(5))\n",
    "display(predictions_ms.head(5))\n",
    "\n",
    "predictions_ms = predictions_ms.reset_index()"
   ],
   "id": "4f6c9e7c9896ec6f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Compare the global to unique predictors:",
   "id": "4b6eb804d36e7325"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "multi_series_mse = multi_series_mse.set_index('levels')\n",
    "multi_series_mse.columns = ['multi_series_mse']\n",
    "\n",
    "results = pd.concat((uni_series_mse, multi_series_mse), axis=1)\n",
    "results['improvement'] = results.eval('uni_series_mse - multi_series_mse')\n",
    "results['improvement_(%)'] = 100 * results.eval('(uni_series_mse - multi_series_mse) / uni_series_mse')\n",
    "results = results.round(2)\n",
    "\n",
    "display(results.style.bar(subset=['improvement_(%)'], align='mid', color=['#d65f5f', '#5fba7d']))\n",
    "display(results[['improvement', 'improvement_(%)']].agg(['mean', 'min', 'max']))\n"
   ],
   "id": "e707edfbc97b19f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Using global predict:",
   "id": "9df87716f241d9ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plotPredictedVsActual(predictions_input, instrument):\n",
    "    df_preds = predictions_input.copy().rename(columns={\"index\": \"time\"})\n",
    "    df_inst_preds = df_preds[df_preds[\"level\"] == instrument]\n",
    "    days_predicted = df_inst_preds[\"time\"]\n",
    "    df_actual = data[[instrument]].copy()\n",
    "    df_actual[\"time\"] = df_actual.index\n",
    "    df_actual = df_actual.rename(columns={instrument: \"actual\"})\n",
    "    df_actual = df_actual[df_actual[\"time\"].isin(days_predicted)]\n",
    "    df_plot = df_inst_preds.merge(df_actual, on=\"time\")\n",
    "    display(df_plot)\n",
    "    # Step 4: Plot\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(df_plot[\"time\"], df_plot[\"actual\"], label=\"Actual Price\", color='dodgerblue')\n",
    "    plt.plot(df_plot[\"time\"], df_plot[\"pred\"], label=\"Predicted Price\", color='orange')\n",
    "    plt.title(f\"Predicted vs Actual Prices for {instrument}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plotPredictedVsActual(predictions_ms, \"inst_0\")\n"
   ],
   "id": "657e76ad5f0ed96e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Above looks decent but also a bit shite",
   "id": "563d2747747d9ad7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Now going to try adding more greeks to use in the model as exogenous features**\n",
   "id": "4565205a014be51"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get exog features in a dictionary\n",
    "greeksFilePaths = sorted(glob.glob(\"./greeks/greeksData_750Days/*.npy\"))\n",
    "feature_names = [os.path.splitext(os.path.basename(f))[0] for f in greeksFilePaths]\n",
    "\n",
    "exog_array = np.stack([np.load(f) for f in greeksFilePaths], axis=-1)\n",
    "\n",
    "exog_dict = {\n",
    "    f\"inst_{i}\": pd.DataFrame(exog_array[:, i, :], columns=feature_names)\n",
    "    for i in range(exog_array.shape[1])\n",
    "}\n",
    "\n",
    "print(f\"{colourOrangeBold}Built exog_dict with {len(exog_dict)} instruments, each shape {exog_dict['inst_0'].shape}{colourReset}\")\n",
    "print(\"Features:\", feature_names)"
   ],
   "id": "f449dfa1292a514",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Create our forecaster including these exogenous features\n",
   "id": "e101943263366c0f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "forecaster_ms_greeks = ForecasterRecursiveMultiSeries(\n",
    "    regressor           = GradientBoostingRegressor(random_state=8523),\n",
    "    lags                = 20,\n",
    "    transformer_series  = StandardScaler(),\n",
    "    transformer_exog    = StandardScaler(),  # Standardise the Greek features\n",
    ")\n",
    "\n",
    "cv = TimeSeriesFold(\n",
    "    initial_train_size = len(data_train) + len(data_val),\n",
    "    steps              = 1,\n",
    "    refit              = True\n",
    ")\n",
    "\n",
    "# Backtest\n",
    "multi_series_mse_greeks, predictions_ms_greeks = backtesting_forecaster_multiseries(\n",
    "    forecaster = forecaster_ms_greeks,\n",
    "    series     = data,\n",
    "    exog       = exog_dict,\n",
    "    levels     = list(data.columns),\n",
    "    cv         = cv,\n",
    "    metric     = \"mean_squared_error\"\n",
    ")\n",
    "\n",
    "display(multi_series_mse_greeks.head())\n",
    "display(predictions_ms_greeks.head())\n",
    "\n",
    "multi_series_mse_greeks = multi_series_mse_greeks.rename(columns={\"mean_squared_error\": \"multi_series_mse_greeks\"})\n",
    "predictions_ms_greeks = predictions_ms_greeks.reset_index()\n",
    "predictions_ms_greeks = predictions_ms_greeks.rename(columns={\"index\": \"time\"})"
   ],
   "id": "dc2e6679297e239f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Compare new predictions and plot new predictions vs Actual",
   "id": "f83573d4fbf536c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "multi_series_mse_greeks = multi_series_mse_greeks.set_index(\"levels\")\n",
    "# Join them\n",
    "results_greeks = pd.concat((multi_series_mse, multi_series_mse_greeks), axis=1)\n",
    "\n",
    "# Calculate improvements\n",
    "results_greeks['improvement'] = results_greeks.eval('multi_series_mse - multi_series_mse_greeks')\n",
    "results_greeks['improvement_(%)'] = 100 * results_greeks['improvement'] / results_greeks['multi_series_mse']\n",
    "results_greeks = results_greeks.round(2)\n",
    "\n",
    "# Display the goods\n",
    "display(results_greeks.style.bar(subset=['improvement_(%)'], align='mid', color=['#d65f5f', '#5fba7d']))\n",
    "display(results_greeks[['improvement', 'improvement_(%)']].agg(['mean', 'min', 'max']))"
   ],
   "id": "1fc850b6b58bb204",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plotPredictedVsActual(predictions_ms_greeks, \"inst_0\")",
   "id": "f67753b0a9e2b189",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Now to try and predict log returns instead of prices:",
   "id": "f40bad423fd26527"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "logReturns_np = np.load(\"./greeks/greeksData_750Days/LogReturns_lookback=1_750_day_data.npy\")\n",
    "logReturns = pd.DataFrame(logReturns_np)\n",
    "\n",
    "logReturns.columns = [f\"inst_{i}\" for i in range(logReturns.shape[1])]\n",
    "\n",
    "print(f\"{colourOrangeBold}Log returns shape = {colourReset}{logReturns.shape}\")"
   ],
   "id": "a02461f962ff2fef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Create exogenous data (now with prices, laggedPrices and without logReturns (logReturns is used in predictions by lag))",
   "id": "dc81321bb19c041c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_paths = sorted(glob.glob(\"./greeks/greeksData_750Days/*.npy\"))\n",
    "\n",
    "feature_paths = [\n",
    "    p for p in all_paths\n",
    "    if 'LogReturns' not in os.path.basename(p)\n",
    "]\n",
    "\n",
    "feature_names = [\n",
    "    os.path.splitext(os.path.basename(p))[0]\n",
    "    for p in feature_paths\n",
    "]\n",
    "\n",
    "exog_array = np.stack([np.load(p) for p in feature_paths], axis=-1)\n",
    "\n",
    "price_array = data.to_numpy()\n",
    "exog_array_with_price = np.concatenate(\n",
    "    [exog_array, price_array[:, :, np.newaxis]],\n",
    "    axis=-1\n",
    ")\n",
    "feature_names.append(\"price\")\n",
    "\n",
    "exog_dict_for_logReturns = {\n",
    "    f\"inst_{i}\": pd.DataFrame(\n",
    "        exog_array_with_price[:, i, :],\n",
    "        columns=feature_names\n",
    "    )\n",
    "    for i in range(exog_array_with_price.shape[1])\n",
    "}\n",
    "\n",
    "print(f\"Built exog_dict_for_logReturns with {len(exog_dict_for_logReturns)} instruments, each shape {exog_dict_for_logReturns['inst_0'].shape}\")\n",
    "print(\"Features:\", feature_names)"
   ],
   "id": "8cba823d6c19dab1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now split our data",
   "id": "99986046fdd15dfe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "logReturns_train = data.loc[TRAIN_START:TRAIN_END - 1].copy()\n",
    "logReturns_val   = data.loc[TRAIN_END:VAL_END - 1].copy()\n",
    "logReturns_test  = data.loc[VAL_END:].copy()\n",
    "\n",
    "print(f\"Train days      : {logReturns_train.index.min()} --- {logReturns_train.index.max()}  (n={len(logReturns_train)})\")\n",
    "print(f\"Validation days : {logReturns_val.index.min()} --- {logReturns_val.index.max()}  (n={len(logReturns_val)})\")\n",
    "print(f\"Test days       : {logReturns_test.index.min()} --- {logReturns_test.index.max()}  (n={len(logReturns_test)})\")"
   ],
   "id": "fe7a0bfaffb4d8b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now create this prediction model",
   "id": "2e1e8bc9d0c81765"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "forecaster_ms_greeks_logReturns = ForecasterRecursiveMultiSeries(\n",
    "    regressor           = GradientBoostingRegressor(random_state=8523),\n",
    "    lags                = 20,\n",
    "    transformer_series  = None,\n",
    "    transformer_exog    = StandardScaler(),  # Standardise the Greek features\n",
    ")\n",
    "\n",
    "cv = TimeSeriesFold(\n",
    "    initial_train_size = len(logReturns_train) + len(logReturns_val),\n",
    "    steps              = 1,\n",
    "    refit              = True\n",
    ")\n",
    "\n",
    "# Backtest\n",
    "multi_series_mse_greeks_logReturns, predictions_ms_greeks_logReturns = backtesting_forecaster_multiseries(\n",
    "    forecaster = forecaster_ms_greeks_logReturns,\n",
    "    series     = logReturns,\n",
    "    exog       = exog_dict_for_logReturns,\n",
    "    levels     = list(logReturns.columns),\n",
    "    cv         = cv,\n",
    "    metric     = \"mean_squared_error\"\n",
    ")\n",
    "\n",
    "display(multi_series_mse_greeks_logReturns.head())\n",
    "display(predictions_ms_greeks_logReturns.head())\n",
    "\n",
    "multi_series_mse_greeks_logReturns = multi_series_mse_greeks_logReturns.rename(columns={\"mean_squared_error\": \"multi_series_mse_greeks_logReturns\"})\n",
    "predictions_ms_greeks_logReturns = predictions_ms_greeks_logReturns.reset_index()\n",
    "predictions_ms_greeks_logReturns = predictions_ms_greeks_logReturns.rename(columns={\"index\": \"time\"})\n",
    "\n",
    "print(f\"{colourOrangeBold} Average MSE across instruments: {multi_series_mse_greeks_logReturns['multi_series_mse_greeks_logReturns'].mean()}\")"
   ],
   "id": "26791fc42cb2e779",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And now plot our predicted log returns against the actual:",
   "id": "67454f6c1775c197"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plotPredictedVsActualLogReturns(predictions_input, instrument):\n",
    "    df_preds = predictions_input.copy().rename(columns={\"index\": \"time\"})\n",
    "    df_inst_preds = df_preds[df_preds[\"level\"] == instrument]\n",
    "    days_predicted = df_inst_preds[\"time\"]\n",
    "    df_actual = logReturns[[instrument]].copy()\n",
    "    df_actual[\"time\"] = df_actual.index\n",
    "    df_actual = df_actual.rename(columns={instrument: \"actual\"})\n",
    "    df_actual = df_actual[df_actual[\"time\"].isin(days_predicted)]\n",
    "    df_plot = df_inst_preds.merge(df_actual, on=\"time\")\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(df_plot[\"time\"], df_plot[\"actual\"], label=\"Actual LogReturn\", color='dodgerblue')\n",
    "    plt.plot(df_plot[\"time\"], df_plot[\"pred\"], label=\"Predicted LogReturn\", color='orange')\n",
    "    plt.title(f\"Predicted vs Actual LogReturns for {instrument}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Log Return\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plotPredictedVsActualLogReturns(predictions_ms_greeks_logReturns, \"inst_0\")"
   ],
   "id": "890cc9d5f0542eda",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " We will consider this an optimal result for prediction and a baseline for future reference",
   "id": "14a725d6b5e78a7f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Convert predicted log returns to be the predicted next price. We can then compare this logReturns predicting model against our pricePredicting model/s",
   "id": "53f895626bbf1e40"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "price_predict_from_logReturns = predictions_ms_greeks_logReturns.copy()\n",
    "\n",
    "# Rename predicted log return column temporarily\n",
    "price_predict_from_logReturns = price_predict_from_logReturns.rename(columns={\"pred\": \"predicted_logReturn\"})\n",
    "\n",
    "# Reconstruct predicted price using previous day's actual price\n",
    "def reconstruct_price(row):\n",
    "    instrument = row['level']\n",
    "    time = row['time']\n",
    "    if time == 0:\n",
    "        return np.nan  # Can't compute for first day\n",
    "    prev_price = data.loc[time - 1, instrument]\n",
    "    log_return = row['predicted_logReturn']\n",
    "    return prev_price * np.exp(log_return)\n",
    "\n",
    "# Apply the function to reconstruct price\n",
    "price_predict_from_logReturns[\"pred\"] = price_predict_from_logReturns.apply(reconstruct_price, axis=1)\n",
    "\n",
    "# Drop the original log return prediction\n",
    "price_predict_from_logReturns = price_predict_from_logReturns.drop(columns=[\"predicted_logReturn\"])\n",
    "\n",
    "display(price_predict_from_logReturns)"
   ],
   "id": "ab72e39c23395131",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Compare",
   "id": "51f6e0f28fd56d84"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Add actual prices from your price DataFrame (`data`)\n",
    "price_predict_from_logReturns[\"actual\"] = price_predict_from_logReturns.apply(\n",
    "    lambda row: data.loc[row[\"time\"], row[\"level\"]],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "predictions_ms_greeks[\"actual\"] = predictions_ms_greeks.apply(\n",
    "    lambda row: data.loc[row[\"time\"], row[\"level\"]],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "mse_logReturns = price_predict_from_logReturns.groupby(\"level\", group_keys=False).apply(\n",
    "    lambda g: mean_squared_error(g[\"actual\"], g[\"pred\"]),\n",
    "    include_groups=False\n",
    ").reset_index(name=\"mse_logReturns\")\n",
    "\n",
    "mse_greeks = predictions_ms_greeks.groupby(\"level\", group_keys=False).apply(\n",
    "    lambda g: mean_squared_error(g[\"actual\"], g[\"pred\"]),\n",
    "    include_groups=False\n",
    ").reset_index(name=\"mse_greeks\")\n",
    "\n",
    "# Merge and compute improvements\n",
    "mse_comparison = pd.merge(mse_logReturns, mse_greeks, on=\"level\")\n",
    "mse_comparison[\"improvement\"] = mse_comparison[\"mse_greeks\"] - mse_comparison[\"mse_logReturns\"]\n",
    "mse_comparison[\"improvement_(%)\"] = 100 * mse_comparison[\"improvement\"] / mse_comparison[\"mse_logReturns\"]\n",
    "mse_comparison = mse_comparison.round(4)\n",
    "\n",
    "# Style output\n",
    "display(mse_comparison.set_index(\"level\").style.bar(\n",
    "    subset=[\"improvement_(%)\"], align=\"mid\", color=[\"#d65f5f\", \"#5fba7d\"]\n",
    "))\n",
    "\n",
    "# Summary stats\n",
    "display(mse_comparison[[\"improvement\", \"improvement_(%)\"]].agg([\"mean\", \"min\", \"max\"]))\n"
   ],
   "id": "bb1c71d47387b967",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Mostly big improvements from predicting prices to predicting log returns.\n",
   "id": "86f4229959eddfb8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Having the model retrain every time will exceed the 10 minute running length in the competition, we need to modify. A few options:\n",
    "- Change the regressor to HistGradientBoostingRegressor\n",
    "- Turn off refit\n",
    "- Somehow change refit to not refit everytime but instead every nth time"
   ],
   "id": "f3aca1955aafb82"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "HIST GRADIENT BOOSTING:",
   "id": "a483298cf6ba7ff0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "forecaster_ms_greeks_logReturns_hgbr = ForecasterRecursiveMultiSeries(\n",
    "    regressor           = HistGradientBoostingRegressor(random_state=8523),\n",
    "    lags                = 20,\n",
    "    transformer_series  = None,\n",
    "    transformer_exog    = StandardScaler(),  # Standardise the Greek features\n",
    ")\n",
    "\n",
    "cv = TimeSeriesFold(\n",
    "    initial_train_size = len(logReturns_train) + len(logReturns_val),\n",
    "    steps              = 1,\n",
    "    refit              = True\n",
    ")\n",
    "\n",
    "# Backtest\n",
    "multi_series_mse_greeks_logReturns_hgbr, predictions_ms_greeks_logReturns_hgbr = backtesting_forecaster_multiseries(\n",
    "    forecaster = forecaster_ms_greeks_logReturns_hgbr,\n",
    "    series     = logReturns,\n",
    "    exog       = exog_dict_for_logReturns,\n",
    "    levels     = list(logReturns.columns),\n",
    "    cv         = cv,\n",
    "    metric     = \"mean_squared_error\"\n",
    ")\n",
    "\n",
    "display(multi_series_mse_greeks_logReturns_hgbr.head())\n",
    "display(predictions_ms_greeks_logReturns_hgbr.head())\n",
    "\n",
    "multi_series_mse_greeks_logReturns_hgbr = multi_series_mse_greeks_logReturns_hgbr.rename(columns={\"mean_squared_error\": \"multi_series_mse_greeks_logReturns_hgbr\"})\n",
    "predictions_ms_greeks_logReturns_hgbr = predictions_ms_greeks_logReturns_hgbr.reset_index()\n",
    "predictions_ms_greeks_logReturns_hgbr = predictions_ms_greeks_logReturns_hgbr.rename(columns={\"index\": \"time\"})\n",
    "\n",
    "print(f\"{colourOrangeBold} Average MSE across instruments: { multi_series_mse_greeks_logReturns_hgbr['multi_series_mse_greeks_logReturns_hgbr'].mean()}\")"
   ],
   "id": "fbc32a0377ad3eca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Compare HGBR to GBR (just using their log returns mse from now on because we will be predicting log returns)\n",
    ":"
   ],
   "id": "f64e12e8e322b535"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def compareModelToBaseline(new_model_mse, baselineModel = multi_series_mse_greeks_logReturns_hgbr):\n",
    "    # Join them\n",
    "    results = pd.merge(\n",
    "        baselineModel,\n",
    "        new_model_mse,\n",
    "        on=\"levels\",\n",
    "    )\n",
    "\n",
    "    # Rename for clarity\n",
    "    results.columns = [\"level\", \"BASELINE_MSE\", \"NEW_MSE\"]\n",
    "\n",
    "    # Calculate improvement of new over GBR\n",
    "    results[\"improvement\"] = results[\"BASELINE_MSE\"] - results[\"NEW_MSE\"]\n",
    "    results[\"improvement_(%)\"] = 100 * results[\"improvement\"] / results[\"BASELINE_MSE\"]\n",
    "\n",
    "    # Round and display\n",
    "    results = results.round(4)\n",
    "    display(results.style.bar(subset=['improvement_(%)'], align='mid', color=['#d65f5f', '#5fba7d']))\n",
    "    display(results[['improvement', 'improvement_(%)']].agg(['mean', 'min', 'max']))\n",
    "\n",
    "compareModelToBaseline(multi_series_mse_greeks_logReturns_hgbr, multi_series_mse_greeks_logReturns)"
   ],
   "id": "a88bf95a7b58bade",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For some reason the HGBR is better than the GBR so lets use that as our new baseline\n",
   "id": "a89d469934c71729"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"HGBR:\")\n",
    "plotPredictedVsActualLogReturns(predictions_ms_greeks_logReturns_hgbr, \"inst_0\")\n",
    "print(\"GBR:\")\n",
    "plotPredictedVsActualLogReturns(predictions_ms_greeks_logReturns, \"inst_0\")"
   ],
   "id": "48118bfb28701447",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now a model without refit",
   "id": "c8db8c9a7c6d17f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "forecaster_ms_greeks_logReturns = ForecasterRecursiveMultiSeries(\n",
    "    regressor           = GradientBoostingRegressor(random_state=8523),\n",
    "    lags                = 20,\n",
    "    transformer_series  = None,\n",
    "    transformer_exog    = StandardScaler(),  # Standardise the Greek features\n",
    ")\n",
    "\n",
    "cv = TimeSeriesFold(\n",
    "    initial_train_size = len(logReturns_train) + len(logReturns_val),\n",
    "    steps              = 1,\n",
    "    refit              = False\n",
    ")\n",
    "\n",
    "# Backtest\n",
    "multi_series_mse_greeks_logReturns_noRefit, predictions_ms_greeks_logReturns_noRefit = backtesting_forecaster_multiseries(\n",
    "    forecaster = forecaster_ms_greeks_logReturns,\n",
    "    series     = logReturns,\n",
    "    exog       = exog_dict_for_logReturns,\n",
    "    levels     = list(logReturns.columns),\n",
    "    cv         = cv,\n",
    "    metric     = \"mean_squared_error\"\n",
    ")\n",
    "\n",
    "display(multi_series_mse_greeks_logReturns_noRefit.head())\n",
    "display(predictions_ms_greeks_logReturns_noRefit.head())\n",
    "\n",
    "multi_series_mse_greeks_logReturns_noRefit = multi_series_mse_greeks_logReturns_noRefit.rename(columns={\"mean_squared_error\": \"multi_series_mse_greeks_logReturns_noRefit\"})\n",
    "predictions_ms_greeks_logReturns_noRefit = predictions_ms_greeks_logReturns_noRefit.reset_index()\n",
    "predictions_ms_greeks_logReturns_noRefit = predictions_ms_greeks_logReturns_noRefit.rename(columns={\"index\": \"time\"})\n",
    "\n",
    "print(f\"{colourOrangeBold} Average MSE across instruments: {multi_series_mse_greeks_logReturns_noRefit['multi_series_mse_greeks_logReturns_noRefit'].mean()}\")"
   ],
   "id": "f1f77eec7dfea665",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Compare this to the best model:",
   "id": "e08b51adc4703103"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Comparing new model of 'GBR no refit' VS 'HGBR with refit'\")\n",
    "compareModelToBaseline(multi_series_mse_greeks_logReturns_noRefit, multi_series_mse_greeks_logReturns_hgbr)"
   ],
   "id": "d56a64ecafb320f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Ok so its clear then that we should use HGBR as our primary choice. Now time for:",
   "id": "b3687a8f3cffaeed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Adding some rolling window features\n",
    "Allowed stats are:\n",
    "- mean\n",
    "- std\n",
    "- min\n",
    "- max\n",
    "- sum\n",
    "- median\n",
    "- ratio_min_max\n",
    "- coef_variation\n",
    "- ewm"
   ],
   "id": "da3e5f9eae3c5b2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "window_features = RollingFeatures(\n",
    "    stats           = ['min', 'max'],\n",
    "    window_sizes    = 10,\n",
    ")\n",
    "\n",
    "window_features_forecaster = ForecasterRecursiveMultiSeries(\n",
    "    regressor = HistGradientBoostingRegressor(random_state=8523),\n",
    "    transformer_series  = None,\n",
    "    transformer_exog    = StandardScaler(),\n",
    "    lags                = 20,\n",
    "    window_features     = window_features,\n",
    ")\n",
    "\n",
    "window_features_forecaster.dropna_from_series = True\n",
    "\n",
    "cv = TimeSeriesFold(\n",
    "    initial_train_size  =len(logReturns_train) + len(logReturns_val),\n",
    "    steps               =1,\n",
    "    refit               =True\n",
    ")\n",
    "\n",
    "multi_series_mse_window, predictions_tuned_window = backtesting_forecaster_multiseries(\n",
    "    forecaster  = window_features_forecaster,\n",
    "    series      = logReturns,\n",
    "    exog        = exog_dict_for_logReturns,\n",
    "    levels      = list(logReturns.columns),\n",
    "    cv          = cv,\n",
    "    metric      = 'mean_squared_error',\n",
    ")\n",
    "\n",
    "multi_series_mse_window = multi_series_mse_window.rename(columns={\"mean_squared_error\": \"multi_series_mse_window\"})\n",
    "predictions_tuned_window = predictions_tuned_window.reset_index()\n",
    "predictions_tuned_window = predictions_tuned_window.rename(columns={\"index\": \"time\"})\n",
    "\n",
    "print(\"Window features: \", window_features.stats)\n",
    "print(\"Window length:   \", window_features.window_sizes)\n",
    "print(\n",
    "    f\"{colourOrangeBold} Average MSE across instruments: {multi_series_mse_window['multi_series_mse_window'].mean()}\")"
   ],
   "id": "88b3bb84723e0e49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Ive tried a bunch of different combos, just min and max is the best.\n",
    "\n",
    "Also tried different scaler combinations. The best is\n",
    "series transformer = None\n",
    "exog transformer =  StandardScaler()"
   ],
   "id": "be8999bc34628269"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Compare this to our baseline HGBR:",
   "id": "32e4f08ee2c0f99a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Comparing new model of 'GBR with window' VS 'HGBR with refit'\")\n",
    "compareModelToBaseline(multi_series_mse_window, multi_series_mse_greeks_logReturns_hgbr)"
   ],
   "id": "5ee07da4f5e1f1b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# HyperParameterTuning",
   "id": "408e1b90e05391ed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### To get the most out of our model, tune the params.\n",
    "\n",
    "Params to tune:\n",
    "- Lags\n",
    "- Learning Rate\n",
    "- max_depth (max depth of tree in the ensemble) # Decided to not do\n",
    "- max_iter (number of trees in the ensemble) # Decided to not do"
   ],
   "id": "d0381e6ffd17f77d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def search_space(trial):\n",
    "    return {\n",
    "        'lags': trial.suggest_categorical('lags', [7, 14]),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "    }"
   ],
   "id": "f3021a49db44c5ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "window_features = RollingFeatures(\n",
    "    stats           = ['min', 'max'],\n",
    "    window_sizes    = 10,\n",
    ")\n",
    "\n",
    "forecaster_ms_tuning = ForecasterRecursiveMultiSeries(\n",
    "    regressor           = HistGradientBoostingRegressor(random_state=8523),\n",
    "    lags                = 20, # Should be overwritten by the search space\n",
    "    transformer_exog    = StandardScaler(),\n",
    "    transformer_series  = None,\n",
    "    encoding            = 'ordinal',\n",
    "    window_features     = window_features,\n",
    ")\n",
    "\n",
    "cv_search = TimeSeriesFold(\n",
    "    initial_train_size  = len(logReturns_train) + len(logReturns_val),\n",
    "    steps               = 1,\n",
    "    refit               = False\n",
    ")"
   ],
   "id": "3fe270d7f6472350",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# For bayesian search we need to change the shape of our exogs\n",
    "def flatten_exog_dict_to_wide_df(exog_dict):\n",
    "    dfs = []\n",
    "    for level, df in exog_dict.items():\n",
    "        renamed = df.copy()\n",
    "        renamed.columns = [f\"{level}__{col}\" for col in df.columns]\n",
    "        dfs.append(renamed)\n",
    "\n",
    "    # All have same number of rows, so just concat by columns\n",
    "    return pd.concat(dfs, axis=1)\n",
    "\n",
    "# Apply it\n",
    "exog_wide = flatten_exog_dict_to_wide_df(exog_dict_for_logReturns)\n",
    "print(\"exog_wide.shape:\", exog_wide.shape)  # Should be (750, many columns)\n"
   ],
   "id": "c7cc564c2b01b13a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Bayesian search for hyper param tuning\n",
   "id": "1335753f32522e23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from skforecast.model_selection import bayesian_search_forecaster_multiseries\n",
    "from skforecast.exceptions import OneStepAheadValidationWarning\n",
    "from skforecast.exceptions import MissingValuesWarning\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=OneStepAheadValidationWarning)\n",
    "warnings.simplefilter(\"ignore\", category=MissingValuesWarning)\n",
    "forecaster_ms_tuning.dropna_from_series = True\n",
    "\n",
    "results_df, best_trial = bayesian_search_forecaster_multiseries(\n",
    "    forecaster      = forecaster_ms_tuning,\n",
    "    series          = logReturns,\n",
    "    levels          = None,  # All instruments\n",
    "    exog            = exog_wide,\n",
    "    cv              = cv_search,\n",
    "    search_space    = search_space,\n",
    "    n_trials        = 40,\n",
    "    metric          = 'mean_squared_error',\n",
    "    return_best     = True, # results_df will contain the best fitting forecaster, params and metrics.\n",
    "    show_progress   = True,\n",
    ")\n",
    "\n",
    "best_idx = results_df['mean_squared_error__average'].idxmin()\n",
    "best_row = results_df.loc[best_idx]\n",
    "\n",
    "print(\"Best Trial Index:\", best_idx)\n",
    "print(\"Best Params:\", best_row['params'])\n",
    "print(\"Best Lags:\", best_row['lags'])\n",
    "print(\"Best MSE:\", best_row['mean_squared_error__average'])"
   ],
   "id": "64783e535ec645d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "best_params = best_row['params']\n",
    "best_lags = best_row['lags']\n",
    "\n",
    "\n",
    "window_features = RollingFeatures(\n",
    "    stats           = ['min', 'max'],\n",
    "    window_sizes    = 10,\n",
    ")\n",
    "best_forecaster = ForecasterRecursiveMultiSeries(\n",
    "    regressor           = HistGradientBoostingRegressor(random_state=8523, **best_params),\n",
    "    transformer_series  = None,\n",
    "    transformer_exog    = StandardScaler(),\n",
    "    lags                = best_lags,\n",
    "    window_features     = window_features,\n",
    ")\n",
    "\n",
    "best_forecaster.dropna_from_series = True\n",
    "\n",
    "cv = TimeSeriesFold(\n",
    "    initial_train_size  = len(logReturns_train) + len(logReturns_val),\n",
    "    steps               = 1,\n",
    "    refit               = True\n",
    ")\n",
    "\n",
    "multi_series_mse_tuned, predictions_tuned = backtesting_forecaster_multiseries(\n",
    "    forecaster  = best_forecaster,\n",
    "    series      = logReturns,\n",
    "    exog        = exog_dict_for_logReturns,\n",
    "    levels      = list(logReturns.columns),\n",
    "    cv          = cv,\n",
    "    metric      = 'mean_squared_error',\n",
    ")\n",
    "\n",
    "multi_series_mse_tuned = multi_series_mse_tuned.rename(columns={\"mean_squared_error\": \"multi_series_mse_tuned\"})\n",
    "predictions_tuned = predictions_tuned.reset_index()\n",
    "predictions_tuned = predictions_tuned.rename(columns={\"index\": \"time\"})\n",
    "\n",
    "print(f\"{colourOrangeBold} Average MSE across instruments: {multi_series_mse_tuned['multi_series_mse_tuned'].mean()}{colourOrangeBold}\")\n"
   ],
   "id": "b0bba2b8b2d2dde4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now compare against our benchmark:",
   "id": "f4a53786f7e11d83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "compareModelToBaseline(multi_series_mse_tuned, multi_series_mse_window)",
   "id": "25b4e39197a372ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "On average better by a tad but not really that much.  =",
   "id": "cffe84610a1d3905"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Plot predicted log returns against actual (and our benchmark too):",
   "id": "639e775d0e449aea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plotPredictedVsActualLogReturns(predictions_tuned, \"inst_0\")\n",
    "plotPredictedVsActualLogReturns(predictions_ms_greeks_logReturns_hgbr, \"inst_0\")"
   ],
   "id": "9131d60514f65476",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Cant really tell a difference",
   "id": "22f292a90b6dd97"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Save our best params to a file:",
   "id": "e08e63d06cf8ac7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "best_forecaster.fit(\n",
    "    series  = logReturns,   # Train the forecaster on all of our data\n",
    "    exog    = exog_dict_for_logReturns,\n",
    ")\n",
    "\n",
    "fitted_scaler = best_forecaster.transformer_exog    # Save our fitted scaler\n",
    "\n",
    "model_package = {\n",
    "    \"forecaster\": best_forecaster,\n",
    "    \"best_params\": best_params,\n",
    "    \"best_lags\": best_lags,\n",
    "    \"transformer_exog\": fitted_scaler,\n",
    "    \"window_features\": window_features,\n",
    "}\n",
    "\n",
    "now = datetime.now()\n",
    "timestamp = now.strftime(\"date-%Y-%m-%d_time-%H-%M-%S\")\n",
    "filepath = f\"./saved models/forecaster_model_{timestamp}.pkl\"\n",
    "\n",
    "# Save to file\n",
    "joblib.dump(\n",
    "    model_package,\n",
    "    filepath,\n",
    ")"
   ],
   "id": "32d767867b13e48c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Some ideas:\n",
    "- Predict vol over an n window and then steps = n as well to have a predictor of future volatilities of instruments for risk\n",
    "- Predict log returns with step 2 together with step 1. then if they oppose say next log return is > 0, and the one after is < 0, might be a bad idea to buy because of the trading tax or something"
   ],
   "id": "5eae97339a9d7c77"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
